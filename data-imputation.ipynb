{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Doramas Báez Bernal - Implementacion de métodos de imputación\n",
    "\n",
    "La tarea a desarrollar consiste en la implementación y comparación de dos métodos de imputación de valores perdidos estudiados en clase. En concreto,vamos a tratar de analizar el método MICE utilizando como estimador la regresión lineal y el método Hot-Deck por muestra más próxima.\n",
    "\n",
    "Para ello, se utilizará el conjunto de datos winequality‐white‐missing.csv que contiene valores perdidos y se comparará los valores obtenidos de las distintas imputaciones utilizando el conjunto winequality‐white.csv. Además, se utilizará el error cuadrático medio como medida de comparacion de los resultados.\n",
    "\n",
    "### Objetivos\n",
    "- Descripción de los datos.\n",
    "- Implementar método MICE.\n",
    "- Implementar método Hot-Deck.\n",
    "- Aplicar los métodos desarrollados en clase.\n",
    "- Comparar los resultados."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import random \n",
    "from sklearn import linear_model, metrics \n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4898 entries, 0 to 4897\nData columns (total 12 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         4400 non-null   float64\n 1   volatile acidity      4431 non-null   float64\n 2   citric acid           4411 non-null   float64\n 3   residual sugar        4434 non-null   float64\n 4   chlorides             4387 non-null   float64\n 5   free sulfur dioxide   4398 non-null   float64\n 6   total sulfur dioxide  4429 non-null   float64\n 7   density               4413 non-null   float64\n 8   pH                    4415 non-null   float64\n 9   sulphates             4372 non-null   float64\n 10  alcohol               4412 non-null   float64\n 11  quality               4898 non-null   object \ndtypes: float64(11), object(1)\nmemory usage: 459.3+ KB\n"
     ]
    }
   ],
   "source": [
    "datos = pd.read_csv('data/winequality-white-missing.csv', sep=';', dtype={'quality':object})\n",
    "datos_original = pd.read_csv('data/winequality-white.csv', sep=';', dtype={'quality':object})\n",
    "datos.info()"
   ]
  },
  {
   "source": [
    "Se dispone de un conjunto de datos que indica la calidad de unos vinos. Para ello, disponemos de ciertas caracteristicas que pueden definir a los vinos como son el azucar residual, el acido citrico o la cantidad de alcohol. En general, el conjunto de datos consta de 10 características de valores reales y una caracteristica utilizada para indicar la calidad del vino, esta no es continua, pero puede ser tratada como valores real a la hora de realizar los cálculos. \n",
    "\n",
    "Otro aspecto que se puede observar es que el dataset consta de 4898 muestras. Sin embargo, se puede observar la existencia de columnas que contienen valores menores, como puede ser la columna \"fixed acidity\". Esto significa, que hay existencia de valores perdidos para esa columna, por lo tanto, se deberá decidir que hacer con estos valores perdidos. En este caso, se va a aplicar diversas tecnicas de imputacion para intentar obtener un conjunto de datos lo mayor posible.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## MICE ALGORITHM\n",
    "\n",
    "1\\. Se realiza una imputación simple en cada variable con valores perdidos.\n",
    "\n",
    "2\\. Se seleciona una variable x con valor perdido y se retorna a valor perdido.\n",
    "\n",
    "3\\. Se realiza una estimacion mediante regresion lineal, utilizando el resto de variables.\n",
    "\n",
    "4\\. El valor perdido de x, se sustituye por la estimacion obtenida en el paso 3.\n",
    "\n",
    "5\\. Se repite los pasos para todas las variables con valores perdidos (un ciclo).\n",
    "\n",
    "6\\. Repetir un determinado número de ciclos.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Mice(dataset,iterations):\n",
    "    datos_Mice = dataset.copy()\n",
    "    colums_names = list(datos_Mice.columns)\n",
    "\n",
    "    # Se calcula la media para la imputacion de valores perdidos\n",
    "    for i in range(11):\n",
    "            variables_perdidas = dataset.iloc[:,i].isnull() # Se obtienen valores nulos del conjunto de datos original\n",
    "            media_variables = datos_Mice.loc[~variables_perdidas,colums_names[i]].mean()\n",
    "            datos_Mice.loc[variables_perdidas,colums_names[i]] = media_variables\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Prediccion por regresion lineal\n",
    "        for j in range(11):\n",
    "            variables_perdidas = dataset.iloc[:,j].isnull() \n",
    "            datos_Mice.loc[variables_perdidas,colums_names[j]] = np.nan # Paso 2 MICE\n",
    "\n",
    "            matrix_perdidos = datos_Mice.loc[variables_perdidas].values\n",
    "            matrix_no_perdidos = datos_Mice.loc[~variables_perdidas].values\n",
    "\n",
    "            x_perdidos = np.delete(matrix_perdidos,j,axis=1)\n",
    "            x = np.delete(matrix_no_perdidos,j,axis=1)\n",
    "            y = matrix_no_perdidos[:,j]\n",
    "\n",
    "            modelo_regresion = linear_model.LinearRegression()\n",
    "            modelo_regresion.fit(x,y)   \n",
    "\n",
    "            variables_pred = modelo_regresion.predict(x_perdidos)  \n",
    "            datos_Mice.loc[variables_perdidas,colums_names[j]] = variables_pred\n",
    "    \n",
    "    \n",
    "        \n",
    "    return datos_Mice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El error medio en la columna fixed acidity es 0.3757166302748024\nEl error medio en la columna volatile acidity es 0.009578597026662396\nEl error medio en la columna citric acid es 0.010867231189791634\nEl error medio en la columna residual sugar es 3.2950084324021143\nEl error medio en la columna chlorides es 0.0002190964280252457\nEl error medio en la columna free sulfur dioxide es 229.56159758464275\nEl error medio en la columna total sulfur dioxide es 953.962289072321\nEl error medio en la columna density es 6.80636695602238e-07\nEl error medio en la columna pH es 0.011872297720247196\nEl error medio en la columna sulphates es 0.011400034090210214\nEl error medio en la columna alcohol es 0.2983388316017732\n"
     ]
    }
   ],
   "source": [
    "res_mice = Mice(datos,20)\n",
    "colums_names = list(res_mice.columns)\n",
    "\n",
    "for i in range(11):\n",
    "    variables_perdidas_originales = datos.iloc[:,i].isnull()\n",
    "    mse = metrics.mean_squared_error(res_mice.loc[variables_perdidas_originales,colums_names[i]],                datos_original.loc[variables_perdidas_originales,colums_names[i]] )\n",
    "\n",
    "    print(\"El error medio en la columna \" + str(colums_names[i]) + \" es \" + str(mse))\n",
    "\n",
    "        "
   ]
  },
  {
   "source": [
    "## Hot-Deck algorithm\n",
    "\n",
    "Consiste en sustituir los valores perdidos por el valor que posee la muestra más proxima considerando todos los atributos excepto el que contiene valores perdidos.\n",
    "- Genera siempre el mismo valor\n",
    "- No es necesario estimar parámetros\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def Hot_Deck(dataset):\n",
    "    datos_Hot_Deck = dataset.copy()\n",
    "    colums_names = list(dataset.columns)\n",
    "\n",
    "    rows_with_nan = datos_Hot_Deck[datos_Hot_Deck.isnull().any(axis=1)]\n",
    "    \n",
    "    rows_without_nan = datos_Hot_Deck.dropna()\n",
    "\n",
    "    for i in range(len(rows_with_nan)): \n",
    "\n",
    "        row_null = rows_with_nan.iloc[i,:].isnull()\n",
    "        row_aux = rows_with_nan.iloc[i,:]\n",
    "\n",
    "        row_clean = row_aux.loc[~row_null].values.reshape(1,-1)\n",
    "        \n",
    "        matrix_without_nan = rows_without_nan.loc[:,~row_null].values\n",
    "        \n",
    "\n",
    "        x = scipy.spatial.distance.cdist(row_clean, matrix_without_nan, 'euclidean')\n",
    "        result = np.where(x == np.amin(x))\n",
    "\n",
    "        for x_loop in range(11):\n",
    "            if row_null[x_loop] == True:\n",
    "                rows_with_nan.iloc[i,x_loop] = rows_without_nan.iloc[result[1][0],x_loop]\n",
    "                \n",
    "            \n",
    "    res = rows_with_nan.append(rows_without_nan)\n",
    "    return res  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "El error medio en la columna fixed acidity es 1.2949397590361449\nEl error medio en la columna volatile acidity es 0.015336241970021416\nEl error medio en la columna citric acid es 0.022559342915811086\nEl error medio en la columna residual sugar es 26.992516163793105\nEl error medio en la columna chlorides es 0.0004246712328767123\nEl error medio en la columna free sulfur dioxide es 310.1065\nEl error medio en la columna total sulfur dioxide es 1639.9962686567164\nEl error medio en la columna density es 2.664641701030923e-06\nEl error medio en la columna pH es 0.034687370600414075\nEl error medio en la columna sulphates es 0.02003555133079848\nEl error medio en la columna alcohol es 1.4119821673525383\n"
     ]
    }
   ],
   "source": [
    "res_hot_deck = Hot_Deck(datos)\n",
    "colums_names = list(res_mice.columns)\n",
    "\n",
    "for i in range(11):\n",
    "    variables_perdidas_originales = datos.iloc[:,i].isnull()\n",
    "    mse = metrics.mean_squared_error(res_hot_deck.loc[variables_perdidas_originales,colums_names[i]],                datos_original.loc[variables_perdidas_originales,colums_names[i]] )\n",
    "\n",
    "    print(\"El error medio en la columna \" + str(colums_names[i]) + \" es \" + str(mse))"
   ]
  },
  {
   "source": [
    "## Imputacion por sustitucion por la media\n",
    "\n",
    "Uno de los métodos con los que vamos a comparar los algoritmos implementados anteriormente es la imputacion por media, así poder cuantificar la calidad de estos.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_mean(dataset):\n",
    "    datos1 = dataset.copy()\n",
    "    colums_names = list(datos1.columns)\n",
    "\n",
    "    for i in range(11):\n",
    "        variables_perdidas = datos1.iloc[:,i].isnull() \n",
    "        media_alcohol = datos1.loc[~variables_perdidas,colums_names[i]].mean() \n",
    "        datos1.loc[variables_perdidas,colums_names[i]] = media_alcohol\n",
    "\n",
    "    return datos1\n"
   ]
  },
  {
   "source": [
    "## Imputacion por regresión lineal \n",
    "\n",
    "Otro de los métodos con los que compararemos el algoritmo Mice y Hot_Deck, será la imputación por regresión lineal. Esta como veremos más adelante, dará unos resultados muy satisfactorios. Sin embargo, no es posible realizarlo en un entorno real, ya que, necesita no tener valores nulos excepto en la columna a imputar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regresion(dataset):\n",
    "    datos1 = dataset.copy()\n",
    "    colums_names = list(dataset.columns)\n",
    "\n",
    "    for x_loop in range(11):\n",
    "        datos1[colums_names[x_loop]] = datos[colums_names[x_loop]]\n",
    "        variables_perdidas = datos1[colums_names[x_loop]].isnull()\n",
    "\n",
    "\n",
    "        matrix_perdidos = datos1.loc[variables_perdidas].values\n",
    "        matrix_no_perdidos = datos1.loc[~variables_perdidas].values\n",
    "        \n",
    "\n",
    "        x_perdidos = np.delete(matrix_perdidos,x_loop,axis=1)\n",
    "        x = np.delete(matrix_no_perdidos,x_loop,axis=1)\n",
    "        y = matrix_no_perdidos[:,x_loop]\n",
    "\n",
    "        modelo_regresion = linear_model.LinearRegression()\n",
    "        modelo_regresion.fit(x,y)   \n",
    "\n",
    "        variables_pred = modelo_regresion.predict(x_perdidos)  \n",
    "        datos1.loc[variables_perdidas,colums_names[x_loop]] = variables_pred\n",
    "\n",
    "    return datos1  \n"
   ]
  },
  {
   "source": [
    "## Comparar los resultados\n",
    "\n",
    "En esta sección, se compararan los resultados de los errores medios de los métodos anteriores y se intentará argumentar las diferencias que se producen entre los mismos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error medio en la columna fixed acidity para el algoritmo mice es 0.3757166302748024\nError medio en la columna fixed acidity para el algoritmo hot_deck es 1.2949397590361449\nError medio en la columna fixed acidity para el algoritmo mean es 0.8310362388028355\nError medio en la columna fixed acidity para el algoritmo regresion lineal es 0.30715519384607953\n===========================================================\nError medio en la columna volatile acidity para el algoritmo mice es 0.009578597026662396\nError medio en la columna volatile acidity para el algoritmo hot_deck es 0.015336241970021416\nError medio en la columna volatile acidity para el algoritmo mean es 0.011386864703054672\nError medio en la columna volatile acidity para el algoritmo regresion lineal es 0.009409758339209361\n===========================================================\nError medio en la columna citric acid para el algoritmo mice es 0.010867231189791634\nError medio en la columna citric acid para el algoritmo hot_deck es 0.022559342915811086\nError medio en la columna citric acid para el algoritmo mean es 0.011884959761715051\nError medio en la columna citric acid para el algoritmo regresion lineal es 0.01081131899746422\n===========================================================\nError medio en la columna residual sugar para el algoritmo mice es 3.2950084324021143\nError medio en la columna residual sugar para el algoritmo hot_deck es 26.992516163793105\nError medio en la columna residual sugar para el algoritmo mean es 24.325632886514416\nError medio en la columna residual sugar para el algoritmo regresion lineal es 1.3742711525332068\n===========================================================\nError medio en la columna chlorides para el algoritmo mice es 0.0002190964280252457\nError medio en la columna chlorides para el algoritmo hot_deck es 0.0004246712328767123\nError medio en la columna chlorides para el algoritmo mean es 0.00026968134679845613\nError medio en la columna chlorides para el algoritmo regresion lineal es 0.0002143304347389852\n===========================================================\nError medio en la columna free sulfur dioxide para el algoritmo mice es 229.56159758464275\nError medio en la columna free sulfur dioxide para el algoritmo hot_deck es 310.1065\nError medio en la columna free sulfur dioxide para el algoritmo mean es 263.5917005831333\nError medio en la columna free sulfur dioxide para el algoritmo regresion lineal es 161.22923799168598\n===========================================================\nError medio en la columna total sulfur dioxide para el algoritmo mice es 953.962289072321\nError medio en la columna total sulfur dioxide para el algoritmo hot_deck es 1639.9962686567164\nError medio en la columna total sulfur dioxide para el algoritmo mean es 2144.715774608096\nError medio en la columna total sulfur dioxide para el algoritmo regresion lineal es 861.5700345354069\n===========================================================\nError medio en la columna density para el algoritmo mice es 6.80636695602238e-07\nError medio en la columna density para el algoritmo hot_deck es 2.664641701030923e-06\nError medio en la columna density para el algoritmo mean es 7.693090791033641e-06\nError medio en la columna density para el algoritmo regresion lineal es 2.3498099120817987e-07\n===========================================================\nError medio en la columna pH para el algoritmo mice es 0.011872297720247196\nError medio en la columna pH para el algoritmo hot_deck es 0.034687370600414075\nError medio en la columna pH para el algoritmo mean es 0.0209212983290277\nError medio en la columna pH para el algoritmo regresion lineal es 0.007941792497865122\n===========================================================\nError medio en la columna sulphates para el algoritmo mice es 0.011400034090210214\nError medio en la columna sulphates para el algoritmo hot_deck es 0.02003555133079848\nError medio en la columna sulphates para el algoritmo mean es 0.01261208059035362\nError medio en la columna sulphates para el algoritmo regresion lineal es 0.010752038101958286\n===========================================================\nError medio en la columna alcohol para el algoritmo mice es 0.2983388316017732\nError medio en la columna alcohol para el algoritmo hot_deck es 1.4119821673525383\nError medio en la columna alcohol para el algoritmo mean es 1.4226523559446482\nError medio en la columna alcohol para el algoritmo regresion lineal es 0.12999466274473498\n===========================================================\n"
     ]
    }
   ],
   "source": [
    "res_mice = Mice(datos,20)\n",
    "    \n",
    "res_hot_deck = Hot_Deck(datos)\n",
    "res_mean = imputation_mean(datos) \n",
    "res_lineal = linear_regresion(datos_original)\n",
    "\n",
    "colums_names = list(datos.columns)\n",
    "\n",
    "for i in range(11):\n",
    "    variables_perdidas_originales = datos.iloc[:,i].isnull()\n",
    "\n",
    "    mse_mice = metrics.mean_squared_error(res_mice.loc[variables_perdidas_originales,colums_names[i]],                datos_original.loc[variables_perdidas_originales,colums_names[i]] )\n",
    "\n",
    "    mse_deck = metrics.mean_squared_error(res_hot_deck.loc[variables_perdidas_originales,colums_names[i]],                datos_original.loc[variables_perdidas_originales,colums_names[i]] )\n",
    "\n",
    "    mse_mean = metrics.mean_squared_error(res_mean.loc[variables_perdidas_originales,colums_names[i]],                datos_original.loc[variables_perdidas_originales,colums_names[i]] )\n",
    "\n",
    "    mse_lineal = metrics.mean_squared_error(res_lineal.loc[variables_perdidas_originales,colums_names[i]],                datos_original.loc[variables_perdidas_originales,colums_names[i]] )\n",
    "\n",
    "    print(\"Error medio en la columna \" + str(colums_names[i]) + \" para el algoritmo mice es \" + str(mse_mice))\n",
    "    print(\"Error medio en la columna \" + str(colums_names[i]) + \" para el algoritmo hot_deck es \" + str(mse_deck))\n",
    "    print(\"Error medio en la columna \" + str(colums_names[i]) + \" para el algoritmo mean es \" + str(mse_mean))\n",
    "    print(\"Error medio en la columna \" + str(colums_names[i]) + \" para el algoritmo regresion lineal es \" + str(mse_lineal))\n",
    "    print(\"===========================================================\")"
   ]
  },
  {
   "source": [
    "- Imputación por Mice: en general, los resultados obtenidos mediante la imputacion son bastante buenos, se puede observar que son similares a la regresion lineal y esto es significativo porque indica que se obtiene buena calidad en los resultados. Además, un aspecto muy importante es que para realizar imputacion mediante el método mice no dependemos de los datos originales.\n",
    "\n",
    "- Imputación por Hot-Deck: se puede observar, que para el algoritmo Hot_Deck, a veces se obtiene unos valores buenos y otras veces no tanto. Esto es debido a que, se ha divido el dataset en dos conjuntos, filas sin valores nulos y filas con valores nulos. Por lo tanto, cuando se realiza el calculo euclideo para calcular las distancias, estamos comparando la fila que tiene un valor nulo con las filas sin valores nulos. Esto provoca, que se pierda una gran cantidad de posibles vecinos y que la distancia minima entre los vecinos pueda ser elevada en algunos casos. No obstante, se puede observar que cuando encuentra vecinos buenos este algoritmo es bastante aceptable aunque un poco peor que el mice. \n",
    "\n",
    "- Imputacion por media: la imputacion por media simple, suele dar unos errores bastante elevados y normalmente se debe descartar utilizar dicho metodo.\n",
    "\n",
    "- Imputación por regresion lineal: este método da los mejores resultados, sin embargo, este es solo aplicable a un caso ideal donde se dispone el conjunto de datos original.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}